{"cells":[{"cell_type":"markdown","source":["# ENSF-612 Quiz 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6455a526-1422-42c6-9ba4-23276e21eb0e"}}},{"cell_type":"markdown","source":["### Question 1\n\n\n1. In Hadoop, each stage involves disk IO. As a result, jobs are run at the speed of disk and not at the speed of the processor. Apache spark executes faster than Hadoop because Apache spark provides in-memory computing and its design is intended to transform the in-memory data, which drastically reduces the time spent on disk.\n\n2. False. RDDs are immutable.\n\n3. False. Transformation is lazily evaluated, not action.\n\n4. Spark automatically creates closures for functions and global variables used by the workers. These closures are sent for every task. This ends up having large lookup table for workers. Also, the changes done to global variables are not sent back to driver.\n   \n   Broadcast and accumulator variables address these probems. Broadcast variables help in efficiently sending large read-only values to workers by shipping to each worker once instead of with each task. Accumulators help in aggregating values from workers back to the driver, which are useful in implementing parallel counters and sum."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f070d6c0-aab8-4cd4-b588-db3145973739"}}},{"cell_type":"markdown","source":["### Question 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dbbec01-7ac8-4411-b7a7-197cc9701acd"}}},{"cell_type":"markdown","source":["I created a dummy file, and uploaded it at `dbfs:/FileStore/quiz_2/data.csv`. I load this file into pyspark dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65cdbf83-3ba6-4de8-aa10-1fe2281180b1"}}},{"cell_type":"code","source":["def read_CSV_to_DF(filepath):\n  \"\"\"\n  Read a csv file into a spark dataframe\n  \"\"\"\n  df = (spark.read\n        .option(\"multiline\", \"true\")\n        .option(\"quote\", '\"')\n        .option(\"header\", \"true\")\n        .option(\"escape\", \"\\\\\")\n        .option(\"escape\", '\"')\n        .csv(filepath)\n        )\n  \n  return df\n\n# creating the dataframe\ndf = read_CSV_to_DF('/FileStore/quiz_2/data.csv')\n\n# updating the datatype of columns of dataframe\ndf = df.withColumn('QuestionId', df['QuestionId'].cast('int'))\ndf = df.withColumn('Score', df['Score'].cast('int'))\n\n# show the contents of the file\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f59b3c8-74d2-43d6-99a0-84989911331a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------------+-----+\n|QuestionId|HasAcceptedAnswer|Score|\n+----------+-----------------+-----+\n|         1|             True|   25|\n|         2|            False|   26|\n|         3|            False|   24|\n|         4|             True|   11|\n|         5|             True|    3|\n|         6|            False|   29|\n|         7|             True|   22|\n|         8|             True|   16|\n|         9|            False|    5|\n|        10|            False|   26|\n|        11|             True|   30|\n|        12|             True|   21|\n|        13|            False|   17|\n|        14|            False|   15|\n|        15|            False|   26|\n|        16|             True|   28|\n|        17|             True|   12|\n|        18|            False|   12|\n|        19|            False|   13|\n|        20|            False|    7|\n+----------+-----------------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------------+-----+\n|QuestionId|HasAcceptedAnswer|Score|\n+----------+-----------------+-----+\n|         1|             True|   25|\n|         2|            False|   26|\n|         3|            False|   24|\n|         4|             True|   11|\n|         5|             True|    3|\n|         6|            False|   29|\n|         7|             True|   22|\n|         8|             True|   16|\n|         9|            False|    5|\n|        10|            False|   26|\n|        11|             True|   30|\n|        12|             True|   21|\n|        13|            False|   17|\n|        14|            False|   15|\n|        15|            False|   26|\n|        16|             True|   28|\n|        17|             True|   12|\n|        18|            False|   12|\n|        19|            False|   13|\n|        20|            False|    7|\n+----------+-----------------+-----+\nonly showing top 20 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Task 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b136c28-248b-49d0-a3eb-181accc34317"}}},{"cell_type":"code","source":["rdd_sumAcceptedScore = df.rdd.filter(lambda x: x['HasAcceptedAnswer'] == \"True\").map(lambda x: (x['HasAcceptedAnswer'] ,x['Score'])).reduceByKey(lambda a, b: a+b).collect()\n\nprint(f'Total score of accepted answers = {rdd_sumAcceptedScore[0][1]}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f936873c-f6c9-4472-b291-913d513ad44f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Total score of accepted answers = 384\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Total score of accepted answers = 384\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Task 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c45cc728-27f1-4a65-881c-8aa11ff41978"}}},{"cell_type":"code","source":["rdd_sumNotAcceptedScore = df.rdd.filter(lambda x: x['HasAcceptedAnswer'] == \"False\").map(lambda x: (x['HasAcceptedAnswer'] ,x['Score'])).reduceByKey(lambda a, b: a+b).collect()\n\nprint(f'Difference of accepted answers and non-accepted answers = {rdd_sumAcceptedScore[0][1] - rdd_sumNotAcceptedScore[0][1]}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af240dcc-1346-4e7e-b2d7-8be6ecf1422b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Difference of accepted answers and non-accepted answers = -120\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Difference of accepted answers and non-accepted answers = -120\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 3\n\n1. While creating the RDD from the text file, no minimum number of partitions is provided, and hence by default two partitions are created. We can supply a bigger value to ensure more partitions are created. By creating more partitions, we can utilize all of the cores available in the cluster.\n    ```\n    fileRDD = sc.textFile(\"BigLog.txt\", 8)\n    ```\n    \n2. We can cache the RDD before doing any transformations, so that RDD can persist in the memory for the next time we query it.\n    ```\n    fileRDD.cache()\n    ```\n\nFinal code would look like below\n```\nfileRDD = sc.textFile(\"BigLog.txt\", 8)\nfileRDD.cache()\n\ndef filter1(record):\n  ...\n\ndef filter2(record):\n  ...\n\n\nresult1RDD = fileRDD.filter(filter1)\nprint(result1RDD.take(5))\nresult2RDD = fileRDD.filter(filter2)\nprint(result2RDD.take(5))\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c557422-1acb-4e9b-80f4-6caccf0484c1"}}},{"cell_type":"markdown","source":["### Question 4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e01a7de4-fb4a-4f20-bce8-99d17c97432c"}}},{"cell_type":"markdown","source":["![Q4_Pipeline.png](/files/quiz_2/Q4_Pipeline.png)\n\n1. We use a supervised machine learning algorithm.\n\n2. Assume that for each of the consumer, we group our data such that we have information on which products were bought, when (month of purchase), and where (region of purchase). And for each consumer, we have their first name, last name, gender, country of origin, and last four digits of credit card.\n\n   We tokenize our consumer data - ie first name, last name, gender, country of origin, the last four digits of credit card - which uniquely identifies a consumer.\n\n   Region, country of origin, and month of buying the product are categorical, so we first convert to numerical using StringIndexer and then encode them using OneHotEncoder.\n\n   We also encode all our products code using OneHotEncoder.\n\n   Since gender is categorical, we encode the gender into 0 and 1 using StringIndexer and then OneHotEncoder.\n\n   For the rest of the tokenized textual data, ie, first name, last name, and last four digits of credit card, we generate TF-IDF matrix. We also use CountVectorizer to generate an extra feature out of length of first name and last name.\n\n   Finally we use VectorAssember to put all our features together and feed it into our ML model.\n\n3. Since we have data of what all products were purchased by consumers, we can easily group the data for each consumer to see which products are frequently bought together. We train our model on existing purchase history to predict the consumer purchase in the next month. The features region and month of purchase play an important part in influencing the output of the ML model. Purchase of the same consumer can be traced using first name, last name, country of origin, and last four digits of credit card."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"374c7df3-1ead-401e-a6d3-4995f9b02102"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Solution","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2182682912467681}},"nbformat":4,"nbformat_minor":0}
